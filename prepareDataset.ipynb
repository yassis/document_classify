{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07c64b5-b3d0-4944-914e-657742515971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 12:54:28.668025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    \n",
    "        Step 1 in the training: we convert the (human-readable) CSV\n",
    "        with training data into number matrices with the appropriate\n",
    "        shape, ready for the actual training of the classifier.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa5038-666b-4e81-a9d2-ee1e34c21afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in\n",
    "datasetInputPath = 'training/raw_dataset/'\n",
    "# df\n",
    "datasetInputFile = 'training/prepared_dataset/aggregated_df.csv'\n",
    "# out\n",
    "trainingDumpFile = 'training/prepared_dataset/training_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47ba434-ec8d-4179-a0ae-f7a96e68e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "labels = []\n",
    "texts = []\n",
    "raw_folds = [fold for fold in  os.listdir(datasetInputPath) if not fold.startswith('.') ]\n",
    "for raw_folder in raw_folds:\n",
    "    raw_files = [file for file in os.listdir(os.path.join(datasetInputPath, raw_folder)) if not file.startswith('.')]\n",
    "    for raw_file in raw_files: \n",
    "        with open(os.path.join(*[datasetInputPath, raw_folder, raw_file]), 'r') as f:\n",
    "            try:\n",
    "                texts.append(''.join([line.strip() for line in f.readlines()]))\n",
    "                labels.append(raw_folder)\n",
    "            except:\n",
    "                print(\"ERROR IN FILE READ {}\".format(os.path.join(*[datasetInputPath, raw_folder, raw_file])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4f0126-a1d4-4903-ac4d-8f8c09399735",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame({\"text\": texts, \"label\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5f4215-94e1-4144-a2a2-c1b9b77da7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "other_texts = []\n",
    "other_labels = []\n",
    "for i in range(100):\n",
    "    other_text = \"\"\n",
    "    ind = randrange(100)\n",
    "    for label in raw_df['label'].unique():\n",
    "        source_text = list(raw_df[raw_df['label'] == label]['text'])[ind]\n",
    "        other_text += ' '.join(source_text.split()[:200])\n",
    "    other_texts.append(other_text)\n",
    "    other_labels.append('other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b8d92a-d058-4493-93a2-438fc98038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df = pd.DataFrame({\"text\": other_texts, \"label\": other_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3bc90a-5ad0-40f5-8d99-dad0b26de7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.concat([raw_df, other_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deffb0c6-d80f-4cda-8ebc-86043e2d453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451697a4-73dd-4cc9-9baa-4e0a46af94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_csv(datasetInputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7eba81a-3711-4299-8a97-421a86e8c620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "technologie      100\n",
       "sport            100\n",
       "graphics         100\n",
       "food             100\n",
       "politics         100\n",
       "other            100\n",
       "business         100\n",
       "entertainment    100\n",
       "medical          100\n",
       "historical       100\n",
       "space            100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791780b3-ce0a-42d8-b344-2c40d6b0afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['text_len'] = raw_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f65c9698-6493-45bc-9822-f03175cf71a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1100.000000\n",
       "mean      3305.712727\n",
       "std       4227.345263\n",
       "min        108.000000\n",
       "25%       1137.500000\n",
       "50%       1726.500000\n",
       "75%       3099.500000\n",
       "90%      10238.600000\n",
       "99%      17314.870000\n",
       "max      51925.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['text_len'].describe(percentiles = [0.25,0.5,0.75,0.9,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e623a2da-d29e-4d78-b45b-026c06f85df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reindent(t, n): \n",
    "    return '\\n'.join('%s%s' % (' ' * n if ix > 0 else '', l) for ix, l in enumerate(t.split('\\n')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4441c5-a0b2-4611-9c5c-f274a890bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading ... done\n",
      "    Tokenizing ... done\n",
      "        tokenizer.word_index             = {'the': 1, 'of': 2, 'to': 3, 'and': 4, 'in': 5} +...\n",
      "        inverseWordIndex                 = {1: 'the', 2: 'of', 3: 'to', 4: 'and', 5: 'in'} +...\n",
      "        sequences[350]                   = [22, 1, 88, 3, 136, 6, 5, 239, 137, 131, 263, 9, 6, 489, 37, 263, 49, 454, 37, 15, 47, 3, 238, 3, 15, 13, 306, 263, 223, 1, 2, 7, 263, 76, 9, 353, 18, 84, 66, 6, 131, 25, 13, 68, 95, 353, 5, 1, 197, 270, 141, 10, 4, 6, 269, 4, 8, 7, 3, 96, 4, 312, 75, 8, 3, 454, 75, 263, 30, 49, 46, 1, 255, 43, 1, 255, 149, 4, 13, 6, 10, 96, 75, 8, 84, 66, 1, 10, 2, 201, 9, 34, 79, 4, 1, 455, 129, 83, 8, 50, 98, 104, 1, 8, 142, 7, 4, 70, 11, 3, 6, 7, 3, 1, 149, 7, 242, 2, 372, 13, 79, 9, 2, 86, 7, 378, 6, 97, 8, 84, 6, 7, 91, 263, 3, 99, 10, 1, 4, 55, 8, 38, 327, 3, 14, 10, 33, 11, 6, 155, 149, 5, 75, 4, 75, 59, 55, 5, 3, 116, 416, 75, 391, 93, 14, 4, 75, 5, 47, 110, 16, 6, 131, 8, 3, 6, 4, 110, 16, 416, 7, 6, 130, 4, 38, 96, 75, 6]\n",
      "        [\n",
      "            inverseWordIndex[i]\n",
      "            for i in sequences[350]\n",
      "        ]                                = ['from', 'the', '3', 'to', 'design', 'a', 'in', 'graphic', 'good', 'great', 'business', 'that', 'a', 'across', 'their', 'business', 'can', 'increase', 'their', 'by', 'up', 'to', 'according', 'to', 'by', 'with', 'every', 'business', 'home', 'the', 'of', 'for', 'business', 'no', 'that', 'companies', 'are', 'now', 'into', 'a', 'great', 'but', 'with', 'so', 'many', 'companies', 'in', 'the', 'same', 'market', 'both', 'on', 'and', 'a', 'image', 'and', 'is', 'for', 'to', 'make', 'and', 'whether', 'your', 'is', 'to', 'increase', 'your', 'business', 'or', 'can', 'all', 'the', 'right', 'if', 'the', 'right', 'way', 'and', 'with', 'a', 'on', 'make', 'your', 'is', 'now', 'into', 'the', 'on', 'of', 'users', 'that', 'they', 'like', 'and', 'the', 'rate', 'between', 'them', 'is', '1', 'just', 'how', 'the', 'is', 'used', 'for', 'and', 'when', 'it', 'to', 'a', 'for', 'to', 'the', 'way', 'for', '7', 'of', 'spending', 'with', 'like', 'that', 'of', 'use', 'for', 'having', 'a', 'well', 'is', 'now', 'a', 'for', 'any', 'business', 'to', 'get', 'on', 'the', 'and', 'there', 'is', 'more', 'likely', 'to', 'be', 'on', 'which', 'it', 'a', 'long', 'way', 'in', 'your', 'and', 'your', 'out', 'there', 'in', 'to', 'high', 'images', 'your', 'graphics', 'should', 'be', 'and', 'your', 'in', 'up', 'such', 'as', 'a', 'great', 'is', 'to', 'a', 'and', 'such', 'as', 'images', 'for', 'a', 'still', 'and', 'more', 'make', 'your', 'a']\n",
      "        texts[350]                       = \"Learn from the Experts: 3 Simple Tricks To Design A Standout BrandPublished in Branding, Graphic designA good brand equals great business. Businesses that present a brand consistently across their business can increase their revenue by up to 23 percent,  according to statistics cited by Forbes. With nearly every business expert continually driving home the importance of branding for business success, itâ€™s no surprise that companies are now plugging millions into building a great brand. But with so many companies competing in the same market both on and offline, designing a standout image and brand is essential for businesses looking to  make promotion and advertising easier. Whether your aim is to increase your business value or drive consumer recognition, brand building can tick all the right boxes if done the right way and with a core focus on visuals.Make Your Social Media Presence Visually AppealingSocial media is now ingrained into the consumer buying process. On Instagram, 90 percent of users follow businesses that they like and the engagement rate between them is currently 1.90 percent, indicating just how often the platform is used for viewing and interacting. When it comes to video marketing â€” a  key content marketing trend for 2020 â€” social media continues to lead the way, accounting for 28.7 percent of total video ad spending.With studies like PWCâ€™s Retail 2017 survey showing that 59 percent of consumers use social media for everyday inspiration, having a well-curated social media account is now a necessity for any business looking to get on the map and stay there. Visual content is more likely to be shared on social media, which means it goes a long way in building your brand and getting your name out there. In addition to high-quality visual images, your graphics should be original and your content delivered in easily summed-up formats, such as infographics. A great idea is to  choose a social media color palette and posting styles, such as grayscale images for a still, serene, and more personal tone.Make Your Brandâ€™s Print Visuals A Priority\"\n",
      "    Padding ... done\n",
      "        [len(s) for s in sequences]      = [60, 555, 38, 41, 303, 1117] + ...\n",
      "        len(sequences)                   = 1100\n",
      "        X.shape                          = (1100, 2000)\n",
      "        type(X)                          = <class 'numpy.ndarray'>\n",
      "        X[350]                           = ... + [ 0  0  0 ... 96 75  6]\n",
      "    Casting as categorical ... done\n",
      "        labelsAsIntArray.shape           = (1100,)\n",
      "        y.shape                          = (1100, 11)\n",
      "        y[:5]                            = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                            [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                            [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                            [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                            [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "        labels[:5]                       = ['technologie', 'sport', 'graphics', 'food', 'politics']\n",
      "        labelLegend                      = {'technologie': 0, 'sport': 1, 'graphics': 2, 'food': 3, 'politics': 4, 'other': 5, 'business': 6, 'entertainment': 7, 'medical': 8, 'historical': 9, 'space': 10}\n",
      "    Splitting dataset ... done\n",
      "        X_train.shape = (737, 2000)\n",
      "        X_test.shape  = (363, 2000)\n",
      "        y_train.shape = (737, 11)\n",
      "        y_test.shape  = (363, 11)\n",
      "    Saving ... done\n",
      "        Saved keys = X_test/X_train/label_legend/label_legend_inverted/max_seq_length/max_words/tokenizer/y_test/y_train\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "verbose = 1\n",
    "# Reading the input file and preparing legend info\n",
    "print('    Reading ... ', end ='')\n",
    "df = pd.read_csv(datasetInputFile)\n",
    "labels = df['label'].tolist()\n",
    "texts = df['text'].tolist()\n",
    "#\n",
    "labelLegend = {v:k for k, v in enumerate(raw_df['label'].unique())}\n",
    "labelLegendInverted = {'%i' % v: k for k,v in labelLegend.items()}\n",
    "labelsAsInt = [labelLegend[x] for x in labels]\n",
    "print('done')\n",
    "\n",
    "\n",
    "# Tokenization of texts\n",
    "print('    Tokenizing ... ', end ='')\n",
    "MAX_NUM_WORDS = 500\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print('done')\n",
    "if verbose:\n",
    "    print('        tokenizer.word_index             = %s +...' % str(dict(list(tokenizer.word_index.items())[:5])))\n",
    "    inverseWordIndex = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    print('        inverseWordIndex                 = %s +...' % str(dict(list(inverseWordIndex.items())[:5])))\n",
    "    print('        sequences[350]                   = %s' % str(sequences[350]))\n",
    "    print('        [')\n",
    "    print('            inverseWordIndex[i]')\n",
    "    print('            for i in sequences[350]')\n",
    "    print('        ]                                = %s' % (\n",
    "        [inverseWordIndex[i] for i in sequences[350]]\n",
    "    ))\n",
    "    print('        texts[350]                       = \"%s\"' % texts[350])\n",
    "\n",
    "# Padding of sequences\n",
    "print('    Padding ... ', end ='')\n",
    "MAX_SEQ_LENGTH = 2000\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)\n",
    "print('done')\n",
    "if verbose:\n",
    "    print('        [len(s) for s in sequences]      = %s + ...' % str([len(s) for s in sequences[:6]]))\n",
    "    print('        len(sequences)                   = %s' % str(len(sequences)))\n",
    "    print('        X.shape                          = %s' % str(X.shape))\n",
    "    print('        type(X)                          = %s' % str(type(X)))\n",
    "    print('        X[350]                           = ... + %s' % str(X[350][285:]))\n",
    "\n",
    "# Switch to categorical form for labels\n",
    "print('    Casting as categorical ... ', end ='')\n",
    "labelsAsIntArray = np.asarray(labelsAsInt)\n",
    "y = to_categorical(labelsAsIntArray)\n",
    "print('done')\n",
    "if verbose:\n",
    "    print('        labelsAsIntArray.shape           = %s' % str(labelsAsIntArray.shape))\n",
    "    print('        y.shape                          = %s' % str(y.shape))\n",
    "    print('        y[:5]                            = %s' % _reindent(str(y[:5]),43))\n",
    "    print('        labels[:5]                       = %s' % str(labels[:5]))\n",
    "    print('        labelLegend                      = %s' % str(labelLegend))\n",
    "\n",
    "print('    Splitting dataset ... ', end ='')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print('done')\n",
    "if verbose:\n",
    "    print('        X_train.shape = %s' % str(X_train.shape))\n",
    "    print('        X_test.shape  = %s' % str(X_test.shape))\n",
    "    print('        y_train.shape = %s' % str(y_train.shape))\n",
    "    print('        y_test.shape  = %s' % str(y_test.shape))\n",
    "    # Respectively: (5043, 300) (2485, 300) (5043, 2) (2485, 2)\n",
    "\n",
    "print('    Saving ... ', end ='')\n",
    "\n",
    "trainingData = {\n",
    "    'X_train': X_train, \n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'max_words': MAX_NUM_WORDS,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'label_legend': labelLegend,\n",
    "    'label_legend_inverted': labelLegendInverted, \n",
    "    'tokenizer': tokenizer,\n",
    "}\n",
    "with open(trainingDumpFile, 'wb') as f:\n",
    "    pickle.dump(trainingData, f)\n",
    "print('done')\n",
    "if verbose:\n",
    "    print('        Saved keys = %s' % '/'.join(sorted(trainingData.keys())))\n",
    "#\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81886fd3-6848-4b36-9b0b-832752813f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
